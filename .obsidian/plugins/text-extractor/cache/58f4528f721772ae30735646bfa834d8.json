{"path":"Year 2/Practical Algorithms/slides/analysis_of_algos_draft_20241031.pdf","text":"Algorithm Analysis S Waqar Nabi and Yiannis Giannakopoulos School of Computing Science, University of Glasgow Motivation – What characteristics do you want your programmed algorithm to have? Correct Readable Reusable Testable Elegant Motivation – What characteristics do you want your programmed algorithm to have? Efficient How long will it take to run? (execution time) How much space (memory) will it take? Motivation – What characteristics do you want your programmed algorithm to have? Correct Readable Reusable Testable Elegant Efficient How long will it take to run? (execution time) How much space (memory) will it take? Motivation – What characteristics do you want your programmed algorithm to have? Correct Readable Reusable Testable Elegant Execution time Number of steps If we assume each “step” takes about the same amount of time, then we can use the steps taken as a proxy for execution time This Photo by Unknown Author is licensed under CC BY Evaluating algorithmic efficiency for adding 𝑛 numbers • Let’s revisit the “empirical” approach we have in fact already taken earlier 1 + 2 + 3 + ⋯ + 𝑛 = We’ve been there, done that... • Unit 1, 4.1, Problem 5: Recalling the experiment • Over to the code: • Note that version A and B are effectively the same algorithm (both use a loop over all elements) but simply use a different type of loop, so the difference is not algorithmic • ... and for the purpose of the type of analysis we wish to carry out, not significant Extending the experiment • Repeat the experiment for different values of 𝑛 • Let’s see if we can find some kind of a trend • More code… • Can you predict the trend by analysing the code, without having to run the code many times to get empirical results? • Yes you can! And that’s what this unit is about 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0 2000000 4000000 6000000 8000000 10000000 12000000Time taken (sec) n Summing n Integers - Version A (for loop) 0 0.2 0.4 0.6 0.8 1 1.2 1.4 0 2000000 4000000 6000000 8000000 10000000 12000000Time taken (sec) n Summing n Integers - Version B (while loop) 0.00E+00 2.00E-06 4.00E-06 6.00E-06 8.00E-06 1.00E-05 1.20E-05 1.40E-05 1.60E-05 0 2000000 4000000 6000000 8000000 10000000 12000000Time taken (sec) n Summing n Integers - Version C (direct formula) Note size of numbers!Takeaways • More than specific performance at specific problem sizes, we are interested in the trends ➢ How does time complexity scale with the problem size? • At small problem sizes, we don’t often get a true picture. ➢ We should consider large problem sizes for such analysis 2.20E-06 2.50E-06 1.20E-06 0.00E+00 5.00E-07 1.00E-06 1.50E-06 2.00E-06 2.50E-06 3.00E-06 A B C N = 10 0.0723049 0.1247723 2.20E-06 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 A B C N = 1 000 000 Experimental (or Empirical) Approach vs. ? Given that: • We are not really interested in exact times for a specific problem size • We are more interested in the trend, that is, how does the time complexity increase as the problem size increases • so that we can make educated decisions about which algorithms to use • Is there another way to approach this problem? Algorithm Analysis • Analyse the time and space efficiency of an algorithm, without reference to specific implementation details (language, hardware, RAM size etc) “Algorithm analysis is a way to compare the time and space efficiency of programs with respect to their possible inputs, but irrespective of other context.”* *https://bradfieldcs.com/algos/analysis/introduction/ Emprical vs Analytical Approach • What we just did with the three versions of summing up numbers was an empirical approach. • We ran experiments with real code, and measured the actual time/space taken. • What is more useful is a more abstract, analytical approach. • Uses a high-level description of the algorithm instead of a specific implementation • Characterizes running time (and space) as a function of the input size (𝒏) • How does my running time 𝑇 𝑛 change, as I increase my input size 𝑛 ? Empirical studies vs Theoretical analysis Empirical studies • It is necessary to implement the algorithm, which may be difficult • Results may not be indicative of the running time on other inputs not included in the experiment • In order to compare two algorithms, the same hardware and software environments must be used • Results may be more accurate, realistic, and relevant to the specific context. Theoretical analysis • Uses a high-level description of the algorithm instead of an implementation • Characterizes running time as a function of the input size (𝑛) • Considers all possible inputs • Allows us to evaluate the speed of an algorithm independently of the hardware/software environment • Results will be indicative. Emprical vs Analytical Approach • In the rest of this lecture, we will focus on the analytical approach • In our problem sessions, we will do a bit of both analytical and empirical analysis. • The questions of the analytical approach are: • What is precisely the property of the algorithm that we want to capture analytically? ➢ (Time) Complexity, as in: “how many steps will it require” ➢ But: best case, worst case, average case? • How do we express it? • How do we calculate it? Best, Average, and Worst Case Complexity Which one to Analyse? Why? Sorting a card deck of 52 cards • Here’s Waqar’s algorithm: 1. ... in fact, I’ll show you... Sorting a card deck of 52 cards • Here’s Waqar’s algorithm: 1. Shuffle the deck 2. If it is not sorted, repeat step 1. 3. Deck is now sorted Best case • 1 step Best case • 1 step Worst Case • 52 x 51 x 50 x ... x 1 = 52! steps 8 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 0 steps 1 500 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 years (1 step / millisec) Best case • 1 step Worst Case* • 52 x 51 x 50 x ... x 1 = 52! steps 8 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 0 steps 1 500 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 years (1 step / millisec) Average Case • 52! / 2 steps 4 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 0 steps 0 750 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 years (1 step / millisec) Assuming that at each reshuffling we generate a new permutation! What shall we work with? • Best case is useful to know, but usually not practical • Average case can often be useful • Typically, as scientists, we want to give guarantees • So: we usually want to estimate the worst case scenario • This worst case estimate of the time complexity (how many steps will it take?) of an algorithm by analysing it (rather than running it) is best represented by the Big O notation Asymptotic Analysis: Big-O Notation Mathematical Representation of Algorithmic Complexity Expressing the Worst Case Complexity So we express the running time (complexity) as a function of problem size (typically represented by 𝑛 ⟼ 𝑇(𝑛) ) That is, the complexity is expressed as a function of 𝑛 We then look at the behaviour of that function at large values of n This is called asymptotic analysis We can’t make assumptions about the size of the problem (e.g. number of cards, size of list of numbers, etc) We want to express the complexity of an algorithm That is, how mich time will it take in the worst case to solve a given problem using that particular algorithm More on “Running Time” 𝑇(𝑛) • We want describe the running time of a program 𝑇, as a function of its input size 𝑛. 𝑛 → 𝑇 𝑛 • What is 𝑛? • “Size of the problem” • Can be a single value; e.g. size of list you are sorting • Can be multiple values as well; e.g. number of nodes and edges in a graph • What is 𝑇(𝑛)? • Meant to represent running time • We use number of primitive operations or “steps” as proxy (recall the “RAM” model) • We calculate it be “counting” the number of steps in a given algorithm for a given 𝑛. • 𝑇(𝑛) can represent best, average, or worst-case, but we usually focus on the worst case. To express how complexity grows with respect to the size of the problem we use the Big-O “O(f(n))” notation Edmund Landau Donald Knuth So what does 𝑂(𝑓(𝑛)) represent? • We make one more simplifying assumption in our complexity analysis: It is only the Order of Growth that interests us (rather than the exact expression T(n) of the running time). • This means we can focus only on the highest-order terms in a formula, and ignore others (we’ll look into this more closely). • To give a quick example • given you found 𝑇(𝑛) for a particular algorithm to be 𝑇(𝑛) = 4𝑛2 + 2𝑛 + 10000 • then the “order of growth” is simply: n2 • This means 𝑇(𝑛) = 4𝑛2 + 2𝑛 + 10000 is of the order 𝑂(𝑛2) Some common “Orders of Growth” (or “Growth Rates”) 𝑂(1) – Constant running time • Cleaning house for a party • No matter how many guests come, cleaning the house prior to their arrival will take the same amount of time • Summing up 𝒏 consecutive numbers using a formula • No matter what the size of 𝒏 is, finding the sum using the Gaussian sum formula 1 + 2 + ⋯ + 𝑛 = 𝑛(𝑛 + 1) 2 takes a fixed number of ”basic” steps/elementary operations (namely 𝑇 𝑛 = 2; one multiplication and one division) 𝑛 (size of problem)𝑇𝑛 (# steps) 𝑂(𝑛) – Linear growth • Passing the roast around the table at a party • If guests are doubled, then wait twice as long to pass it round • Summing up 𝒏 integers using a loop • If you double 𝒏, you will loop 2x more times, and take (roughly) twice as long 𝑛 (size of problem)𝑇𝑛 (# steps) 𝑂(𝑛2) – Quadratic growth • Everyone in the party gives everyone else a hug • for 𝑛 guests, it is 𝑛 − 1 + 𝑛 − 2 + ⋯ + 2 + 1 • equal to (𝑛2 − 𝑛) / 2 • the dominant term here is n2 (we will revisit this concept in a bit) • So this is 𝑂(𝑛2) • Multiply two 𝑛-bit numbers 𝑛 (size of problem)𝑇𝑛 (# steps) 𝑂(2𝑛) – Exponential growth The legend of the rice and the chessboard 𝑂(2𝑛) – Exponential Growth • As the size of the problem increases by 1... • ...the steps required to solve it double (or, more generally, scales up by some other factor > 1) 𝑛 (size of problem)𝑇𝑛 (# steps) 𝑂(log 𝑛) – Logarithmic Growth • The inverse of the exponential function • You have to double the size of the problem before you need one additional step • Because every step halves the remaining problem size • The telephone directory (number against name) problem: • Reduce the size of the problem by half each time • Card sorting • Successively divide pile into halves until you get to individual cards • Start merging the piles, maintaining order as you merge • Start with a 1000 cards → get to single cards in ~10 steps • Start with a 1000,000 cards → get to single cards in ~ 20 steps • Start with a 1000,000,000 cards → get to single cards in ~30 steps 𝑛 (size of problem)𝑇𝑛 (# steps) 𝑂(𝑛!) – Factorial Growth • Grows real quick, real fast! • E.g. our initial naïve card sorting algorithm (“just keep re-shuffling”) • There are 52! possible card permutations n (size of problem)f(n) [Time / steps] Some other common growth rates − 𝑂 𝑛 = 𝑂 𝑛 1/2 • or, more generally, fractional powers: 𝑂 𝑛𝑐 for 0 < 𝑐 < 1 − O(n log n) or quasilinear − O(n3) or cubic Comparing common growth rates 𝒏 𝒍𝒐𝒈 𝒏 𝒏 𝒏 𝒏 𝒍𝒐𝒈 𝒏 𝒏𝟐 𝒏𝟑 𝟐𝒏 𝒏! 8 3 2.83 8 24 64 512 256 40320 16 4 4.00 16 64 256 4096 65536 2.09228E+13 32 5 5.66 32 160 1024 32768 4.29E+09 2.63131E+35 64 6 8.00 64 384 4096 262144 1.84E+19 1.26887E+89 128 7 11.31 128 896 16384 2097152 3.4E+38 3.8562E+215 256 8 16.00 256 2048 65536 16777216 1.16E+77 #NUM! 512 9 22.63 512 4608 262144 1.34E+08 1.3E+154 #NUM! • Roughly: logs < roots < polynomials < exponentials < factorials 𝑙𝑜𝑔𝛼 𝑛 ≺≺ 𝛽 𝑛 ≺≺ 𝑛𝛾 ≺≺ 𝛿𝑛 ≺≺ 𝑛! for any 𝛼, 𝛽, 𝛾, 𝛿 > 1 Graphical representation of Asymptotic Analysis n log n log n2 2n n ! 2 4 6 8 10 50 100 150 200 𝑛 = 1,2, … , 10 Graphical representation of Asymptotic Analysis 𝑛 = 1, … , 2 Note that at small values we don’t see the “problem” with n! or 2n, they seem ok That is why we do an asymptotic analysis: studying the behaviour of the complexity functions at large values of n (Most computing problems are “large”; that’s why we are using computers to solve them!) A Couple of Subtle Points… 1. Underlying “data structure” is critical: • E.g.: finding the median of a list of numbers: • 𝑂(1) if the list is sorted • 𝑂(𝑛 log 𝑛) in general 2. “Fast” vs “slow” quantification strongly dependent on the given application 3. What is a “primitive”, single-step operation? Depends on the underlying hardware architecture and computational model • How long does it really take to add two integers? • In this course: we will try to abstract away from point, but we will very much care about point 1, and to some extent about #3. Determining big-O complexity: 1. Counting primitive operations 2. Expressing them as a function of the problem size → 𝑇(𝑛) 3. Finding the dominant part of that function that represents its growth rate or “big-Oh” complexity Determining big-O complexity: 1. Counting primitive operations 2. Expressing them as a function of the problem size → 𝑇(𝑛) 3. Finding the dominant part of that function that represents its growth rate or “big-Oh” complexity First: A Model of Implementation Hardware • We assume a generic processor, where instructions are executed one after another, with no concurrent operations. • This is called the RAM model • In real-life situations involving processors with parallel processing capabilities (which is most processors now), concurrency considerations need to be taken into account • We aren’t going there in this course, but go here for a quick deep dive This Photo by Unknown Author is licensed under CC BY-SA Identifying Primitive operations (“steps”) in Algorithmic Analysis • Basic computations performed by an algorithm • Low-level instructions commonly found in real computers: arithmetic (+, -, /, x), data movement, control. • Assumed to take a common amount of time • We ignore memory hierarchies, cache hits/misses etc • Identifiable in pseudocode • Largely independent from the programming language • Exact definition not important (We will see why later) • Examples • Fundamental arithmetic operations (ie: addition, multiplication, …) • Value assignment to a variable • Array indexing • Function call • (only the act of calling a function is “primitive”; executing a function can span many primitive operations, and will need to be analysed) • Returning from a method Which of these is a primitive operation? • val = a+b • array2 = sort_ascending(array1) Which of these is a primitive operation? • val = a+b YES • array2 = sort_ascending(array1) NO! Counting primitive operations • By inspecting the pseudocode, we can determine: • the worst-case (ie, maximum) number of primitive operations executed by an algorithm, • as a function of the input size 𝐼𝑛𝑝𝑢𝑡𝑆𝑖𝑧𝑒 → 𝑓 𝑀𝑎𝑥𝑂𝑝s or 𝑛 → 𝑓 𝑇(𝑛) ARRAY-MAX(A) max := A[0] for i = 1 to n-1 if A[i] > max then max := A[i] {increment counter i} return max • Given an Array A of numerical values, find its maximum element: Counting Primitive Operations Example: “ArrayMax” Problem Counting primitive operations • By inspecting the pseudocode, we can determine the maximum number of primitive operations executed by an algorithm, as a function of the input size • Example: find the maximum value in an array A of integers (with indices between 0 and n-1) ARRAY-MAX(A) max := A[0] for i = 1 to n-1 if A[i] > max then max := A[i] {increment counter i} return max Operations 2 assignment and array indexing 55 Counting primitive operations • By inspecting the pseudocode, we can determine the maximum number of primitive operations executed by an algorithm, as a function of the input size • Example: find the maximum value in an array A of integers (with indices between 0 and n-1) ARRAY-MAX(A) max := A[0] for i = 1 to n-1 if A[i] > max then max := A[i] {increment counter i} return max Operations 2 1+n assignment and test In general, the loop header is executed one time more than the loop body 56 Counting primitive operations • By inspecting the pseudocode, we can determine the maximum number of primitive operations executed by an algorithm, as a function of the input size • Example: find the maximum value in an array A of integers (with indices between 0 and n-1) ARRAY-MAX(A) max := A[0] for i = 1 to n-1 if A[i] > max then max := A[i] {increment counter i} return max Operations 2 1+n 2(n-1) array indexing and test 57 Counting primitive operations • By inspecting the pseudocode, we can determine the maximum number of primitive operations executed by an algorithm, as a function of the input size • Example: find the maximum value in an array A of integers (with indices between 0 and n-1) ARRAY-MAX(A) max := A[0] for i = 1 to n-1 if A[i] > max then max := A[i] {increment counter i} return max Operations 2 1+n 2(n-1) 2(n-1) array indexing and assignment 58 Note: Worst case analysis Counting primitive operations • By inspecting the pseudocode, we can determine the maximum number of primitive operations executed by an algorithm, as a function of the input size • Example: find the maximum value in an array A of integers (with indices between 0 and n-1) ARRAY-MAX(A) max := A[0] for i = 1 to n-1 if A[i] > max then max := A[i] {increment counter i} return max Operations 2 1+n 2(n-1) 2(n-1) 2(n-1) assignment and addition 59 Counting primitive operations • By inspecting the pseudocode, we can determine the maximum number of primitive operations executed by an algorithm, as a function of the input size • Example: find the maximum value in an array A of integers (with indices between 0 and n-1) ARRAY-MAX(A) max := A[0] for i = 1 to n-1 if A[i] > max then max := A[i] {increment counter i} return max Operations 2 1+n 2(n-1) 2(n-1) 2(n-1) 1 return 60 Counting primitive operations • By inspecting the pseudocode, we can determine the maximum number of primitive operations executed by an algorithm, as a function of the input size • Example: find the maximum value in an array A of integers (with indices between 0 and n-1) ARRAY-MAX(A) max := A[0] for i = 1 to n-1 if A[i] > max then max := A[i] {increment counter i} return max Operations 2 1+n 2(n-1) 2(n-1) 2(n-1) 1 Total 7n - 2 61 Determining big-O complexity: 1. Counting primitive operations 2. Expressing them as a function of the problem size → 𝑇(𝑛) 3. Finding the dominant part of that function that represents its growth rate or “big-Oh” complexity Estimating running time from counted operations Algorithm ARRAY-MAX executes 7n − 2 primitive operations in the worst case • Worst case being: input array is in ascending order with maximum value at last position; hence max updated on every step As discussed earlier, we assume all steps take the same amount of time*, so the time taken is is directly proportional to the number of steps *Armed with your know-how of Assembly language, you know better! E.g. a = b+c and a = (b+c)/(d+e), both may look like a single step in a high-level language, but the latter will translate to more machine level instructions or “steps”. For such analysis as we are doing now though, these variations can be ignored. Estimating running time from counted operations Algorithm ARRAY-MAX executes 7n − 2 primitive operations in the worst case • Worst case being: input array is in ascending order with maximum value at last position; hence max updated on every step As discussed earlier, we assume all steps take the same amount of time*, so the time taken is is directly proportional to the number of steps So: 𝑇 𝑛 = 7𝑛 − 2 *Armed with your know-how of Assembly language, you know better! E.g. a = b+c and a = (b+c)/(d+e), both may look like a single step in a high-level language, but the latter will translate to more machine level instructions or “steps”. For such analysis as we are doing now though, these variations can be ignored. It gets simpler! The actual asymptotic analysis can be done way more simply than what we just did. We don’t need an exact expression for number steps. We only need an estimate of how quickly the function grows as the problem size increases. That is, we can ignore lower-order terms Or, in other words, apply the 𝑂() operator on 𝑇(𝑛) 65 This Photo by Unknown Author is licensed under CC BY-SA Determining big-O complexity: 1. Counting primitive operations 2. Expressing them as a function of the problem size → 𝑇(𝑛) 3. Finding the dominant part of that function that represents its growth rate or “big-Oh” complexity Thinking about the growth rate of running time • Expressions like 7n-2, or 6x2+2x+124, are “too precise” for the purposes of asymptotic analysis • Context: Analyse the complexity of an algorithm by estimating how the number of steps grow, in the limit as the size of the problem grows → ∞ • For an algorithm like ARRAY-MAX: • It is the linear growth rate of the running time T(n) the intrinsic property of the algorithm that we wish to focus on • For example, the following running times are all (asymptotically) linear: 7𝑛 − 2 = 𝑂(𝑛) 7239𝑛 = 𝑂(𝑛) 9 350 𝑛 + 100 = 𝑂(𝑛) Managing (simplifying) asymptotic analysis • We can simplify the analysis by looking at the behaviour of different factors that we may find in a 𝑇(𝑛) expression: • Constant factors • Linear factors • Power/Exponential/Factorial factors The Insight • Growth rate is not affected by • constant factors • lower-degree terms • That is, at large enough n, the function 𝑇(𝑛) is largely determined by the highest degree term in its expression • So: Big-Oh notation is used to express asymptotic upper bounds This Photo by Unknown Author is licensed under CC BY-SA Ignore constant factors • Example A = 102n + 105 is a linear function A = B + C, where B = 102n is a linear function C = 105 is a constant (impact on growth rate can be ignored) Ignore lower-degree terms • Example D = 105n2 + 108n is a quadratic function E = 105n2 is a quadratic function F = 108n is a lower-degree (linear) term (impact on growth rate can be ignored) D = E + F Key Intuition (informal) Notation 𝑓 𝑥 = 𝑂 𝑔(𝑥 ) Is the asymptotic analogue of traditional ordering relation 𝑓 𝑥 ≤ 𝑔(𝑥) That is,𝑔(𝑥) provides an asymptotic upper-bound to 𝑓(𝑥)","libVersion":"0.3.2","langs":""}